# Preprocesamiento y validación cruzada

## Punto de partida

Habíamos establecido en la clase anterior que queríamos utilizar nuestra data para construir buenos modelos predictivos SUPERVISADOS y para eso utilizamos la regresión lineal. 

$$
y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_ix_i
$$

También, acuérdate que utilizamos una **función de costo** principalmente para evaluar la predicción, la cual era el RMSE (raíz del error cuadrático medio): 


$$
J(\beta) \;=\; \sqrt{\frac{1}{n}\sum_{i=1}^{n}\big(y_i - \hat{y}_i\big)^2}
$$

Asimismo, lo que hicimos en el ejercicio fue construir modelos con el nivel de pobreza (*pobreza*) y el PBI per cápita (*pbi_pc*)

Vamos a expandir nuestro uso de tidymodels.


## Preprocesamiento con recipe

![](images/recipes.png)

Con recipe no solo definimos la variable objetivo y los predictores; también podemos encadenar, de forma ordenada y reproducible, los pasos de preprocesamiento que aplicaremos al conjunto de datos (imputación, codificación, escalado, etc.). 

Vamos a revisar los más recurrente.

### step_impute_mean(): Imputación de NA

![](images/missing.png)

Utilizamos nuestra base de datos:

```{r}
#| message: false
#| warning: false
library(tidyverse)
library(readxl)
library(tidymodels)
data <- read_xlsx("data/AML_2.xlsx")
data<- data |> 
        filter(!is.na(aml_index))
```

Imagina que ahora deseo utilizar  *rule_of_law* como un predictor pero me doy cuenta que tiene valores perdidos. 

```{r}
summary(data$rule_of_law)
```
Hay que identificar si es que efectivamente es posible **imputar**: 

```{r}
library(naniar)
data |> 
  vis_miss()
```

::: callout-note
La **imputación de datos perdidos** es el proceso de reemplazar valores faltantes (NA) por valores plausibles para poder analizar y modelar sin descartar observaciones. Busca preservar el tamaño muestral, reducir sesgos y permitir que los algoritmos funcionen (muchos no aceptan NA).

Guíate con estos criterios. Nos importa analizar por variable.

1.  Porcentaje BAJO (\<5%):Puedes eliminar esas filas sin perder mucha información o imputar.

2.  Porcentaje MEDIO (5-20%): Conviene imputar.

3.  Porcentaje ALTO (\>30%): Esa variable podría ser candidata a eliminarse, salvo que tenga relevancia en el análisis.
:::

La imputación debe aplicarse siempre, **SIEMPRE**, sobre la data de entrenamiento. 

```{r}
set.seed(2025)
index <- initial_split(data)     
training_data <- training(index)  
testing_data <- testing(index)    
```

Entonces:

```{r}
mi_receta <- recipe(aml_index ~ rule_of_law, data = training_data) |> 
              step_impute_mean(rule_of_law)
```


¿Qué es lo que está haciendo esta función? **step_impute_mean()** imputa los valores faltantes de variables numéricas con la media calculada en el conjunto de entrenamiento. Es decir, con este paso, nos va a permitir utilizar TODOS LOS CASOS, los cual al inicio tenía perdidos. Este es uno de los métodos de imputación más simples.

Ahora bien, antes de seguir, si quieres ver qué hace este step a nuestra data, podemos inspeccionarlo **preparando** la receta solo con el conjunto de entrenamiento (ahí la receta “aprende” parámetros como las medias) y luego mirar el entrenamiento ya transformado con juice() y el conjunto de prueba **horneado** con bake(). 

En la jerga de tidymodels, hornear significa aplicar a datos nuevos (p. ej., el test) exactamente las mismas transformaciones que se estimaron con el train —imputar con las medias del train, escalar con sus desvíos, crear las mismas dummies, etc.— sin recalcular nada y sin modificar tu objeto original; bake() simplemente devuelve un tibble ya transformado, listo para predecir. 

Esto aplica tanto en nuestro training data:

```{r}
training_data |> 
  select(rule_of_law)
```

Que ahora se vería así:

```{r}
mi_receta |> 
     prep() |> # es cuando la receta aprende con el train
     juice()   # devuelve el train ya transformado según lo aprendido en prep
```


Como en nuestro testing data, que antes se veía así:


```{r}
testing_data |> 
  select(rule_of_law)
```

Pero luego de aplicar la receta vamos a ver:

```{r}
mi_receta |> 
      prep() |> # es cuando la receta aprende con el train
      bake(new_data=testing_data) # devuelve el test ya transformado según lo aprendido en prep
```


### step_normalize(): Normalizar variables

**Definición**

Estandariza variables numéricas mediante z-score: a cada columna seleccionada le resta su media y la divide por su desviación estándar. 

![](images/zscore.jpeg)

Cuándo usarlo:

- Modelos sensibles a la escala o basados en distancia: k-NN, SVM, regresiones penalizadas (glmnet: Lasso/Ridge/Elastic Net), redes neuronales.

- No es necesario para árboles, random forest o boosting basados en árboles.

Recuerda que debes hacer esto a las variables numéricas. 

Después de normalizar verás:

- Los números cambian de escala: cada predictor numérico queda con media ≈ 0 y desviación estándar ≈ 1. Verás muchos valores negativos (por debajo del promedio) y positivos (por encima).

- Las unidades desaparecen: ya no están en soles, años o %; ahora todos están en “desviaciones estándar”, por eso son comparables entre sí.

- Las formas no se rompen: el orden de los casos no cambia y las correlaciones entre variables se conservan; solo cambia la escala.

- En tablas/gráficos: histogramas centrados en 0; resúmenes tipo mean ~ 0 y sd ~ 1. Si haces scatterplots, la nube luce igual, pero con ejes reescalados.

- En el modelo: algoritmos sensibles a escala (k-NN, SVM, Lasso/Ridge/Elastic Net, PCA, k-means) se vuelven más estables y los coeficientes en modelos lineales son más comparables en magnitud.


**Ejemplo simple**

Veamos lo que hacemos con un ejemplo:

```{r}
df2 <- tibble(
  edad   = c(22, 25, 28, 30, 34, 37, 41, 45, 52, 58),
  sueldo = c(1200, 1350, 1500, 1650, 2100, 2400, 3000, 3500, 4200, 5000)
)
```

Normalizar implica:

```{r}
df2 <- df2 |> 
  mutate(edad_normalizado=(edad-mean(edad))/sd(edad), 
         sueldo_normalizado=(sueldo-mean(sueldo))/sd(sueldo))
df2
```

Entonces, ahora la escala cambia y son comparables:

```{r}
summary(df2$edad_normalizado)
```
```{r}
summary(df2$sueldo_normalizado)
```

**Aplicación con tidymodels**

Esto que hemos hecho manualmente, también lo podemos hacer con tidymodels y la función step_normalize(). Piensa en las variables rule_of_law y cpi_index. 

```{r}
mi_receta <- recipe(aml_index ~ rule_of_law + pbi_pc, data = training_data) |> 
              step_impute_mean(rule_of_law) |> 
              step_normalize(pbi_pc)
```

Vemos nuestro dataset de entrenamiento:

```{r}
mi_receta |> 
     prep() |> # es cuando la receta aprende con el train
     juice()   # devuelve el train ya transformado según lo aprendido en prep
```

Y nuestor dataset de test:

```{r}
mi_receta |> 
      prep() |> # es cuando la receta aprende con el train
      bake(new_data=testing_data) # devuelve el test ya transformado según lo aprendido en prep
```


### step_dummy(): Predictoras categóricas

**Definición**

Para usar variables categóricas en muchos algoritmos de aprendizaje (por ejemplo, regresiones penalizadas como glmnet, SVM, redes, XGBoost), primero hay que convertirlas a variables numéricas. La forma estándar es el one-hot encoding: cada categoría de una variable se transforma en una columna binaria que vale 1 si el registro pertenece a esa categoría y 0 si no. 

![](images/onehot.png)

Por ejemplo, si color tiene valores (rojo, azul, verde), se crean columnas como color_rojo, color_azul y color_verde. Esto permite que el modelo “entienda” información categórica sin asignar números arbitrarios que introducirían un orden falso.

**Ejemplo simple**

Hagamos un ejemplo básico:

```{r}
df <- tibble(
  id     = 1:5,
  edad   = c(15,21,30,57,62),
  sector = c("publico", "privado", "publico", "privado", "privado"))
df
```

Convertir a dummy, significa que vamos a:

```{r}
df |> 
  mutate(sector_public=ifelse(sector=="publico", 1, 0),
         sector_privado=ifelse(sector=="privado", 1, 0))
```

Como ves en este ejemplo, a la derecha hemos creado dos nuevas variables "sector_publico" que coloca un "1" cuando efectivamente el caso pertenece a ese sector y "sector_privado" cuando ocurre lo propio. En caso no se cumpla la condición, existirá un cero. 


**Aplicación con tidymodels**

Ahora bien, podemos hacer esta transformación de una forma más sencilla utilizando recipes. 

```{r}
mi_receta <- recipe(aml_index ~ rule_of_law + pbi_pc + continente, data = training_data) |> 
              step_impute_mean(rule_of_law) |> 
              step_normalize(pbi_pc, rule_of_law) |> 
              step_dummy(continente)
```

Ahora vemos cómo se vería nuestra data de entrenamiento. 

```{r}
mi_receta |> 
     prep() |> # es cuando la receta aprende con el train
     juice()
```


Y nuestra data de testing:

```{r}
mi_receta |> 
      prep() |> # es cuando la receta aprende con el train
      bake(new_data=testing_data) # devuelve el test ya transformado según lo aprendido en prep
```


## Regresión lineal con receta

### Probando la nueva receta

Recapitulamos la receta que tenemos con preprocesamiento:

```{r}
mi_receta
```

Generamos nuestro modelo:

```{r}
mi_modelo_lm <- linear_reg() |> 
                   set_engine("lm")

flujo_ml<-workflow() |> 
            add_recipe(mi_receta) |> 
             add_model(mi_modelo_lm)

modelo_entrenado <- flujo_ml %>% 
                      fit(data = training_data) # Con el de ENTRENAMIENTO!
```

Si deseamos ver los coeficientes (estimates) del modelo podemos solicitarlo con `tidy()`:

```{r}
tidy(modelo_entrenado)
```

**Qué puedes notar en los coeficientes?**


### Evaluamos el modelo con test

Una vez que el modelo ha sido entrenado, el siguiente paso es evaluar su desempeño. La evaluación consiste en medir qué tan bien el modelo logra predecir los valores de la variable de interés, comparando las predicciones con los valores reales. Para ello se utilizan métricas de error, como el RMSE (Root Mean Squared Error), que nos permiten cuantificar la calidad del ajuste y, sobre todo, estimar su capacidad de generalización cuando se aplica a nuevos datos.

Para ello, primero utilizamos el modelo generado para predecir con la nueva data:

```{r}
prediccion_test<-modelo_entrenado |> 
                  predict(testing_data) |> 
                  bind_cols(valor_real=testing_data$aml_index)
prediccion_test
```

Ahora vamos a medir cómo funciona nuestro modelo utilizándolo con data de testeo. Recuerda que en nuestra data de testeo podemos validar en contraste con el valor real.

```{r}
yardstick::rmse(prediccion_test,
    truth = valor_real,
    estimate = .pred)
```

El R cuadrado es otra medida, aunque menos utilizada en machine learning, que dice cuánto es **explicado** por nuestro modelo.

```{r}
rsq(prediccion_test,
    truth = valor_real,
    estimate = .pred)
```

Graficamos:

```{r}
prediccion_test |> 
  ggplot()+
  aes(x = valor_real, y = .pred)+
  geom_point(color = "blue", size = 2) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +  # línea 1:1
  labs(
    x = "Valor real",
    y = "Valor predicho",
    title = "Valores reales vs predicciones"
  ) +
  xlim(0,10)+ ylim(0,10)+
  theme_minimal()
```

## Remuestreo para evaluar el rendimiento: cross-validation

Hasta aquí te había comentado que era necesario separar nuestra data en data de entrenamiento (train) y data de evaluación (test).

![](images/TRAINTEST.png)

Sin embargo, el depender de **un único split** (partición) es un riesgo para nosotros. Si te pones a pensar es como evaluar a un deportista con una sóla carrera. En otras palabras, la "nota" del modelo queda a merced del azar del muestreo. Por eso, se establece que antes de irnos a evaluar al modelo con el testing dataset debe pasar por un proceso de validación. 

Este proceso de validación requeriría hacer un split adicional. Normalmente, en un escenario óptimo, vamos a tener la suficiente cantidad de información para crear una partición más. 

![](images/validation.png)

Sin embargo, lo que usualmente ocurre es que no se cuenta con la suficiente cantidad de información. Esto nos deja una salida: usar una técnica que se llama **cross-validation o validación cruzada**.  

![](images/cv.png)

La idea detrás del remuestreo es usar tu muestra disponible como si fuera el universo para simular muchos “nuevos” conjuntos de datos y así estimar con mayor fiabilidad cómo rendiría un modelo o un estimador fuera de esa muestra. En vez de depender de un único corte train/test (muy sensible al azar), divides o vuelves a muestrear tus datos muchas veces, entrenas y evalúas repetidamente, y promedias los resultados.

Primero debemos crear nuestras particiones utilizando nuestro training data. En este caso vamos a comenzar utilizando sólo cuatro particiones o folds. 

```{r}
set.seed(2025)
folds <- vfold_cv(training_data, v= 4)
```

Si entramos a nuestro primer fold, vemos que hemos divido la data de entrenamiento (59) en dos partes. Una primera que fungirá como data para analizar nuestro modelo (aquí para evitar la confusión con el término entrenamiento, se utiliza la palabra **analysis**) y un fold de 15 casos para el **assess**. 

```{r}
folds$splits[[1]]
```

Para correr nuevamente nuestro modelo utilizando cross-validation necesitamos seguir los mismos pasos del workflow, pero utilizando el fit en las nuevas particiones:

```{r}
mi_modelo_lm <- linear_reg() |> 
                   set_engine("lm")

flujo_ml<-workflow() |> 
            add_recipe(mi_receta) |> 
             add_model(mi_modelo_lm)

modelos_entrenados_cv <- flujo_ml %>% 
                          fit_resamples(resamples = folds) #DIFERENTE
```


Ahora, podemos solicitar las métricas para cada uno de los folds creado:


```{r}
modelos_entrenados_cv |> 
  collect_metrics(summarize = FALSE)
```

Finalmente, solicitamos el promedio:

```{r}
modelos_entrenados_cv |> 
  collect_metrics()
```

Este fit_samples se recomienda hacerlo antes del fit con la data de testeo. De tal forma que antes de la evaluación, ya sabes que tu modelo puede funcionar con distintos subdataset creados a partir de tu data de entrenamiento original. 